---
title: _Chirp!_ Project Report
subtitle: ITU BDSA 2025 Group 8
author:
- "Charlotte Plateig <cpla@itu.dk>"
- "Frederik Hørup <frap@itu.dk>"
- "Marie Johansen <majoh@itu.dk>"
- "Nikolej Lundquist <nivl@itu.dk>"
- "Sara Bagger <salb@itu.dk>"
numbersections: true
---

# Design and Architecture of _Chirp!_

## Domain model

Here comes a description of our domain model.

![Illustration of the _Chirp!_ data model as UML class diagram.](docs/images/domain_model.png)

## Architecture — In the small

## Architecture of deployed application

## User activities

## Sequence of functionality/calls trough _Chirp!_

# Process

## Build, test, release, and deployment

## Team work

## How to make _Chirp!_ work locally

## How to run test suite locally

# Ethics

## License

## LLMs, ChatGPT, CoPilot, and others
ChatGPT and, to a smaller degree, Microsoft Copilot were used during the development of this project.
Let's briefly discuss how these were used and how helpful they were.
<br><br>
Debugging. Since no group members have prior experience with C# and many other concepts of this course,
when particularly tricky errors occured LLMs were frequently used to help debug the problem. However,
we would strive to first try and solve it ourselves. In this use case LLMs were invaluable,
as it saved significant time that would otherwise have been spent googling or asking the TA for help.
<br><br>
Coder's block. As mentioned, this project introduced many new and unfamiliar concepts,
which meant that for some tasks, we would have no clue where to begin.
LLMs were used as a spring board in these cases to get started on a new feature for example.
Specifically, when refactoring to Onion Architecture ChatGPT was used to help translate theory into practice
by suggesting a template for how our project should be organized. This saved a lot of time making decisions
and discussing semantics with the rest of the group, which allowed everyone to focus on their own tasks.
<br><br>
Code generation. Very little code in this project was generated by LLMs and copy/pasted directly,
and in cases where it was, it is marked by comments and/or in the commit message as co-author.
The group feels largely unconfortable using AI generated code and opted to only use it in certain cases.
For example, the GitHub workflows are mostly generated by ChatGPT and modified afterwards.
While we could have taken the time to learn workflow syntax etc.,
we instead chose to rely on ChatGPT which saved a lot of time.
Additionally, smaller snippits were copy/pasted from ChatGPT when debugging.
<br><br>
Overall, the use of LLMs, ChatGPT in particular, has indeed sped up development significantly.
Not so much as a code-generation tool, but as a virtual, on-demand TA that can help
understanding new concepts, getting started on a new task, and debug persistant errors.