---
title: _Chirp!_ Project Report
subtitle: ITU BDSA 2025 Group 8
author:
- "Charlotte Plateig <cpla@itu.dk>"
- "Frederik Hørup <frap@itu.dk>"
- "Marie Johansen <majoh@itu.dk>"
- "Nikolej Lundquist <nivl@itu.dk>"
- "Sara Bagger <salb@itu.dk>"
numbersections: true
---

# Design and Architecture of _Chirp!_

## Domain model

Here comes a description of our domain model.

![Illustration of the _Chirp!_ data model as UML class diagram.](docs/images/domain_model.png)

## Architecture — In the small

## Architecture of deployed application

## User activities

## Sequence of functionality/calls trough _Chirp!_

# Process

## Build, test, release, and deployment

## Team work

## How to make _Chirp!_ work locally

## How to run test suite locally

# Ethics

## License
This project is licensed under the MIT License. The MIT License was chosen since the nuget packages this project uses are either licensed under the MIT License or Apache-2.0 License. Since these licenses are permissive software licenses there does not arise any conflicts using the MIT License. 

## LLMs, ChatGPT, CoPilot, and others
ChatGPT and, to a smaller degree, Microsoft Copilot were used during the development of this project.
Let's briefly discuss how these were used and how helpful they were.
<br><br>
Debugging. Since no group members have prior experience with C# and many other concepts of this course,
when particularly tricky errors occured LLMs were frequently used to help debug the problem. However,
we would strive to first try and solve it ourselves. In this use case LLMs were invaluable,
as they served as a TA guiding us through the debugging process, in cases where Googling was not sufficient and no actual TA was nearby.
<br><br>
Coder's block. As mentioned, this project introduced many new and unfamiliar concepts,
which meant that for some tasks, we would have no clue where to begin.
LLMs were used as a spring board in these cases to get started on a new feature for example.
Specifically, when refactoring to Onion Architecture ChatGPT was used to help translate theory into practice
by suggesting a template for how our project should be organized. This saved a lot of time making decisions
and discussing semantics with the rest of the group, which allowed everyone to focus on their own tasks.
<br><br>
Code generation. Very little code in this project was generated by LLMs and copy/pasted directly,
and in cases where it was, it is marked by comments and/or in the commit message as co-author.
The group feels largely unconfortable using AI generated code and opted to only use it in certain cases.
For example, the GitHub workflows are mostly generated by ChatGPT and modified afterwards.
As these workflows are mostly boilerplate and not part of the application code,
it did not take away from our learning experience but allowed us to prioritize our time and ressources in other areas.
Additionally, smaller snippits were copy/pasted from ChatGPT when debugging.
<br><br>
Overall, the use of LLMs, ChatGPT in particular, has indeed sped up development significantly.
Not so much as a code-generation tool, but as a virtual, on-demand TA that helped
understanding new concepts, getting started on new tasks, and debug persistant errors.
